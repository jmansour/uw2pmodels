{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UW_ENABLE_TIMING\"] = \"1\"\n",
    "import underworld as uw\n",
    "from underworld import function as fn\n",
    "import underworld.visualisation as vis\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "order = int(os.getenv(\"UW_ORDER\",\"2\"))\n",
    "res   = int(os.getenv(\"UW_RESOLUTION\",16))\n",
    "itol  = 1.e-6\n",
    "otol  = 1.e-6\n",
    "\n",
    "soln_name = \"SolDB3d\"\n",
    "\n",
    "do_IO  = bool(int(os.getenv(\"UW_ENABLE_IO\",\"0\")))\n",
    "PREFIX = os.getenv(\"PREFIXSTRING\",\"DefaultRun\")\n",
    "picklename = \"conv_test_results_high_res.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution added: SolA\n",
      "Solution added: SolB\n",
      "Solution added: SolC\n",
      "Solution added: SolCx\n",
      "Solution added: SolDA\n",
      "Solution added: SolDB2d\n",
      "Solution added: SolDB3d\n",
      "Solution added: SolH\n",
      "Solution added: SolKx\n",
      "Solution added: SolKz\n",
      "Solution added: SolM\n",
      "Solution added: SolNL\n"
     ]
    }
   ],
   "source": [
    "# Find all available solutions. \n",
    "# Use ordered dict to preserve alphabetical ordering\n",
    "import collections\n",
    "solns_avail = collections.OrderedDict()\n",
    "for _soln in dir(fn.analytic):\n",
    "    if _soln[0] == \"_\": continue  # if private member, ignore\n",
    "    # get soln class\n",
    "    soln = getattr(fn.analytic,_soln)\n",
    "    # check if actually soln\n",
    "    if issubclass(soln, fn.analytic._SolBase):\n",
    "        print(\"Solution added: {}\".format(_soln))\n",
    "        solns_avail[_soln] = soln\n",
    "soln = solns_avail[soln_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_post_import   = time.time()\n",
    "time_launch_srun   = float(os.getenv(\"TIME_LAUNCH_SRUN\"  ,time_post_import))/1000.\n",
    "time_launch_python = float(os.getenv(\"TIME_LAUNCH_PYTHON\",time_post_import))/1000.\n",
    "uw.timing.start()\n",
    "\n",
    "other_timing = {}\n",
    "other_timing[\"Python_Import_Time\"] = time_post_import - time_launch_python\n",
    "other_timing[\"Container_Launch_Time\"] = time_launch_python - time_launch_srun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_press(press):\n",
    "    intSwarm = uw.swarm.GaussIntegrationSwarm(mesh,3)  # use 3 point gauss swarms for efficiency\n",
    "    av_press = uw.utils.Integral( press, mesh, integrationSwarm=intSwarm, integrationType=None).evaluate()[0]\n",
    "    \n",
    "    return press - av_press\n",
    "\n",
    "def rms_error(numeric, analytic, mesh):\n",
    "    '''\n",
    "    Calculates the rms error.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    abs, abs_scaled: float\n",
    "        The absolute and scaled absolute errors.\n",
    "    '''\n",
    "\n",
    "    delta     = analytic - numeric\n",
    "    delta_dot = fn.math.dot(delta,delta)\n",
    "\n",
    "    analytic_dot = fn.math.dot(analytic,analytic)\n",
    "    \n",
    "    # l2 norms\n",
    "    intSwarm = uw.swarm.GaussIntegrationSwarm(mesh,3)  # use 3 point gauss swarms for efficiency\n",
    "    rms_err_abs = np.sqrt(uw.utils.Integral(    delta_dot, mesh, integrationSwarm=intSwarm, integrationType=None ).evaluate()[0])\n",
    "#     rms_sol_ana = np.sqrt(uw.utils.Integral( analytic_dot, mesh, integrationSwarm=intSwarm, integrationType=None ).evaluate()[0])\n",
    "#     rms_err_sca = rms_err_abs / rms_sol_ana\n",
    "        \n",
    "    return rms_err_abs, 0. #rms_err_sca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35m\n",
      " \n",
      "Pressure iterations:  13\n",
      "Velocity iterations:   4 (presolve)      \n",
      "Velocity iterations:  91 (pressure solve)\n",
      "Velocity iterations:   4 (backsolve)     \n",
      "Velocity iterations:  99 (total solve)   \n",
      " \n",
      "SCR RHS  setup time: 6.8488e-01\n",
      "SCR RHS  solve time: 1.1347e+00\n",
      "Pressure setup time: 5.7641e-03\n",
      "Pressure solve time: 1.0257e+01\n",
      "Velocity setup time: 9.8012e-07 (backsolve)\n",
      "Velocity solve time: 4.1397e-01 (backsolve)\n",
      "Total solve time   : 1.2580e+01\n",
      " \n",
      "Velocity solution min/max: 0.0000e+00/0.0000e+00\n",
      "Pressure solution min/max: 0.0000e+00/0.0000e+00\n",
      " \n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "if order == 1:\n",
    "    els = \"Q1/dQ0\"\n",
    "elif order == 2:\n",
    "    els = \"Q2/dPc1\"\n",
    "else:\n",
    "    raise ValueError(\"Provided system order should be 1 or 2.\")\n",
    "\n",
    "dim           = soln.dim\n",
    "mesh          = uw.mesh.FeMesh_Cartesian(elementType=els, elementRes=(res,)*dim,minCoord=(0.,)*dim,maxCoord=(1.,)*dim)\n",
    "velocityField = uw.mesh.MeshVariable(mesh,dim)\n",
    "pressureField = uw.mesh.MeshVariable(mesh.subMesh, 1)\n",
    "\n",
    "velocityField.data[:]   = (0.,)*dim\n",
    "pressureField.data[:] = 0.\n",
    "\n",
    "bcs = soln.get_bcs(velocityField)\n",
    "# press_set = mesh.subMesh.specialSets[\"Empty\"]\n",
    "# press_set.add(0)\n",
    "# bc_press = uw.conditions.DirichletCondition(pressureField,press_set)\n",
    "visc = soln.fn_viscosity\n",
    "if soln.nonlinear==True:\n",
    "    visc = soln.get_viscosity_nl(vel,press)\n",
    "stokes = uw.systems.Stokes(velocityField, pressureField, fn_viscosity=visc, fn_bodyforce=soln.fn_bodyforce, conditions=[bcs,])\n",
    "solver = uw.systems.Solver(stokes)\n",
    "# if uw.mpi.size==1:\n",
    "#     solver.set_inner_method(\"lu\")\n",
    "solver.set_inner_rtol(itol)\n",
    "solver.set_outer_rtol(otol)\n",
    "# pressureField.data[0] = soln.fn_pressure.evaluate(press_set)\n",
    "# solver.set_penalty(10000.)\n",
    "# if nonlinear, lets first grab a const visc approx soln\n",
    "if soln.nonlinear==True:\n",
    "    stokes.fn_viscosity = 1.\n",
    "    solver.solve()\n",
    "    stokes.fn_viscosity = visc\n",
    "solver.solve()\n",
    "\n",
    "stats=solver.get_stats()\n",
    "solver.print_stats()\n",
    "\n",
    "temperatureField      = uw.mesh.MeshVariable( mesh=mesh, nodeDofCount=1 )\n",
    "temperatureFieldDeriv = uw.mesh.MeshVariable( mesh=mesh, nodeDofCount=1 )\n",
    "temperatureField2     = uw.mesh.MeshVariable( mesh=mesh, nodeDofCount=1 )\n",
    "\n",
    "for index, coord in enumerate(mesh.data):\n",
    "    temperatureField.data[index] = coord[2]\n",
    "temperatureField2.data[:] = temperatureField.data[:]\n",
    "temperatureFieldDeriv.data[:] = 0.\n",
    "\n",
    "kWalls = mesh.specialSets[\"MinK_VertexSet\"] + mesh.specialSets[\"MaxK_VertexSet\"]\n",
    "advdiffBc = uw.conditions.DirichletCondition( variable        = temperatureField,\n",
    "                                              indexSetsPerDof = kWalls )\n",
    "\n",
    "# Create a dummy temperature field.\n",
    "advdiff = uw.systems.AdvectionDiffusion(velocityField=velocityField, phiField=temperatureField, phiDotField=temperatureFieldDeriv, \n",
    "                                        fn_diffusivity=1.,conditions=advdiffBc, allow_non_q1=True)\n",
    "advdiff2 = uw.systems.AdvectionDiffusion(velocityField=velocityField, phiField=temperatureField, \n",
    "                                        fn_diffusivity=1.,conditions=advdiffBc, allow_non_q1=True, method=\"SLCN\")\n",
    "\n",
    "# Create a swarm.\n",
    "swarm = uw.swarm.Swarm( mesh=mesh, particleEscape=True)\n",
    "# Create a layout object, populate the swarm with particles.\n",
    "swarmLayout = uw.swarm.layouts.PerCellSpaceFillerLayout( swarm=swarm, particlesPerCell=40 )\n",
    "swarm.populate_using_layout( layout=swarmLayout )\n",
    "# Create a system to advect the swarm\n",
    "advector = uw.systems.SwarmAdvector( swarm=swarm, velocityField=velocityField, order=2 )\n",
    "\n",
    "store = vis.Store('{}_RT'.format(PREFIX),compress=False)\n",
    "fig = vis.Figure( store, name=\"firstFig\" )\n",
    "fig.append( vis.objects.Points(swarm, pointSize=2, colourBar=False) )\n",
    "fig.append( vis.objects.Surface(mesh, pressureField))\n",
    "fig.append( vis.objects.VectorArrows( mesh, velocityField, scaling=1.0e2))\n",
    "\n",
    "# update \n",
    "# note that the following doesn't make much sense as we're \n",
    "# integrating different objects with different time step \n",
    "# sizes. however, in particular for the swarm advection, \n",
    "# we'd like to use the max timestep as this will stress\n",
    "# the communication overhead the most.\n",
    "dt = advector.get_max_dt()\n",
    "advector.integrate(dt)\n",
    "dt = advdiff.get_max_dt()\n",
    "advdiff.integrate(dt)\n",
    "dt = advdiff2.get_max_dt()\n",
    "advdiff2.integrate(dt)\n",
    "\n",
    "\n",
    "# Save things\n",
    "if do_IO:\n",
    "    meshFileHandle = mesh.save(\"{}_Mesh.h5\".format(PREFIX))\n",
    "\n",
    "    vFH = velocityField.save(\"{}_velocityField.h5\".format(PREFIX))\n",
    "    velocityField.xdmf( \"{}_velocityField\".format(PREFIX), vFH, \"velocity\", meshFileHandle, \"Mesh\" )\n",
    "\n",
    "    swarmFileHandle = swarm.save(\"{}_Swarm.h5\".format(PREFIX))\n",
    "    mH = materialIndex.save(\"{}_materialIndex.h5\".format(PREFIX))\n",
    "    materialIndex.xdmf(\"{}_materialIndex\".format(PREFIX), mH, \"material\", swarmFileHandle, \"Swarm\" )\n",
    "\n",
    "    fig.save()\n",
    "\n",
    "    # load things\n",
    "    # first\t create analogues\n",
    "    mesh_copy = uw.mesh.FeMesh_Cartesian(\n",
    "                                 elementRes  = (res, res, res),\n",
    "                                 minCoord    = (20., 20., 20.),\n",
    "                                 maxCoord    = (33., 33., 33.))\n",
    "\n",
    "    velocityField_copy = uw.mesh.MeshVariable( mesh=mesh_copy,         nodeDofCount=3 )\n",
    "\n",
    "    swarm_copy = uw.swarm.Swarm(mesh = mesh_copy)\n",
    "    materialIndex_copy = swarm_copy.add_variable( dataType=\"int\", count=1 )\n",
    "\n",
    "    # now load data and check loaded versions are identical to originals\n",
    "    mesh_copy.load(\"{}_Mesh.h5\".format(PREFIX))\n",
    "\n",
    "    # test\n",
    "    if not np.allclose(mesh_copy.data, mesh.data):\n",
    "        raise RuntimeError(\"Loaded mesh data does not appear to be identical to previous data.\")\n",
    "    velocityField_copy.load(\"{}_velocityField.h5\".format(PREFIX))\n",
    "    if not np.allclose(velocityField_copy.data, velocityField.data):\n",
    "        raise RuntimeError(\"Loaded velocity data does not appear to be identical to previous data.\")\n",
    "\n",
    "\n",
    "    swarm_copy.load(\"{}_Swarm.h5\".format(PREFIX))\n",
    "\n",
    "    if not np.allclose(swarm_copy.particleCoordinates.data, swarm.particleCoordinates.data):\n",
    "        raise RuntimeError(\"Loaded swarm data does not appear to be identical to previous data.\")\n",
    "    materialIndex_copy.load(\"{}_materialIndex.h5\".format(PREFIX))\n",
    "    if not np.allclose(materialIndex_copy.data, materialIndex.data):\n",
    "        raise RuntimeError(\"Loaded material data does not appear to be identical to previous data.\")\n",
    "\n",
    "uw.timing.stop()\n",
    "module_timing_data_orig = uw.timing.get_data(group_by=\"line_routine\")\n",
    "\n",
    "# write out data\n",
    "filename = \"{}_Res_{}_Nproc_{}_SlurmID_{}\".format(os.getenv(\"SLURM_JOB_NAME\",\"Job\"),res,uw.mpi.size,os.getenv(\"SLURM_JOB_ID\",0000))\n",
    "import json\n",
    "if module_timing_data_orig:\n",
    "    module_timing_data = {}\n",
    "    for key,val in module_timing_data_orig.items():\n",
    "        module_timing_data[key[0]] = val\n",
    "    other_timing[\"Total_Runtime\"] = uw.timing._endtime-uw.timing._starttime\n",
    "    module_timing_data[\"Other_timing\"] = other_timing\n",
    "    module_timing_data[\"Other_data\"]   = { \"res\":res, \"nproc\":uw.mpi.size}\n",
    "    with open(filename+\".json\", 'w') as fp:\n",
    "        json.dump(module_timing_data, fp,sort_keys=True, indent=4)\n",
    "\n",
    "uw.timing.print_table(group_by=\"line_routine\", output_file=filename+\".txt\", display_fraction=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_key = \"Velocity\"\n",
    "pressure_key = \"Pressure\"\n",
    "try:\n",
    "    # try and load existing results\n",
    "    with open(picklename,'rb') as f:\n",
    "        import pickle\n",
    "        soln_results = pickle.load(f)\n",
    "except:\n",
    "    # if failed, it's most prob because the file doesn't \n",
    "    # exist. in this case, create empty dict.\n",
    "    soln_results = collections.OrderedDict()\n",
    "    \n",
    "if (soln_name,order,velocity_key) in soln_results:\n",
    "    err_pre = soln_results[ (soln_name,order,pressure_key) ]\n",
    "    err_vel = soln_results[ (soln_name,order,velocity_key) ]\n",
    "else:\n",
    "    err_pre = collections.OrderedDict()\n",
    "    err_vel = collections.OrderedDict()\n",
    "\n",
    "# grab copy of analytic solutions onto mesh variables. \n",
    "# this is to avoid excessive calls into analytic solutions,\n",
    "# some of which (solH for example) are prohibitively \n",
    "# expensive to calculate \n",
    "velocityFieldA = mesh.add_variable(mesh.dim)\n",
    "pressureFieldA = mesh.subMesh.add_variable(1)\n",
    "velocityFieldA.data[:] = soln.fn_velocity.evaluate(mesh)\n",
    "pressureFieldA.data[:] = soln.fn_pressure.evaluate(mesh.subMesh)\n",
    "pressn = normalise_press(pressureField)\n",
    "pressa = normalise_press(pressureFieldA)\n",
    "\n",
    "err_vel[res] = rms_error( velocityField, velocityFieldA, mesh )\n",
    "err_pre[res] = rms_error( pressn,        pressa,         mesh )\n",
    "\n",
    "soln_results[(soln_name,order,velocity_key)] = err_vel\n",
    "soln_results[(soln_name,order,pressure_key)] = err_pre\n",
    "\n",
    "# record full state back to pickled dict\n",
    "if uw.mpi.rank==0 :\n",
    "    with open(picklename,'wb') as f:\n",
    "        import pickle\n",
    "        f.write(pickle.dumps(soln_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.06640262,  -0.06640404, -10.72265755]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(velocityField-soln.fn_velocity**2).evaluate( (0.5,0.5,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
